# 7 Days of Data Science ğŸ²

Este repositÃ³rio faz parte do desafio [**7 Days of Data Science**](https://7daysofcode.io/matricula/data-science), promovido pela [Alura](https://www.alura.com.br/). O desafio tem como objetivo proporcionar uma experiÃªncia prÃ¡tica com todo o ciclo de anÃ¡lise de dados, desde a coleta atÃ© a modelagem e disponibilizaÃ§Ã£o dos dados.

## ğŸ‘©â€ğŸ’» O que vocÃª vai aprender?
Durante esses 7 dias, vocÃª passarÃ¡ pelas principais etapas da CiÃªncia de Dados:
- **Coleta de Dados**: ImportaÃ§Ã£o e manipulaÃ§Ã£o de bases pÃºblicas.
- **Limpeza e Tratamento**: CorreÃ§Ã£o de inconsistÃªncias e padronizaÃ§Ã£o dos dados.
- **AnÃ¡lise ExploratÃ³ria**: IdentificaÃ§Ã£o de padrÃµes e insights.
- **Modelagem Preditiva**: CriaÃ§Ã£o de modelos para previsÃ£o de dados.
- **Sistema de RecomendacÃ£o**: Desenvolvimento de sugestÃµes personalizadas.
- **Deploy via API**: PublicaÃ§Ã£o de modelos para uso externo.
- **Testes EstatÃ­sticos**: Testes A/B e validaÃ§Ã£o de hipÃ³teses.
- **DocumentaÃ§Ã£o e PortfÃ³lio**: OrganizaÃ§Ã£o dos resultados e apresentaÃ§Ã£o do projeto.

## ğŸ“… Progresso do Desafio

### **Dia 1 - Limpeza e PreparaÃ§Ã£o dos Dados**
Identificamos e tratamos inconsistÃªncias nos dados da base **CEAPS (Cota para ExercÃ­cio da Atividade Parlamentar)**, garantindo que estejam prontos para anÃ¡lise.

### **Dia 2 - AnÃ¡lise ExploratÃ³ria**
Com os dados limpos, iniciamos uma anÃ¡lise exploratÃ³ria para compreender padrÃµes e relaÃ§Ãµes entre variÃ¡veis.

### **Dia 3 - Modelagem Preditiva**
Criamos um modelo preditivo para estimar valores futuros com base nos dados histÃ³ricos.

### **Dia 4 - Sistema de RecomendacÃ£o**
Desenvolvemos um modelo de recomendaÃ§Ã£o para sugerir produtos ou serviÃ§os baseados em comportamento.

### **Dia 5 - Deploy via API**
Publicamos nosso modelo por meio de uma API, permitindo que outros sistemas acessem os insights gerados.

### **Dia 6 - Testes A/B e HipÃ³teses**
Aplicamos testes estatÃ­sticos para validar decisÃµes baseadas em dados.

### **Dia 7 - DocumentaÃ§Ã£o e PortfÃ³lio**
Finalizamos o projeto, organizamos os arquivos e preparamos a documentaÃ§Ã£o para o portfÃ³lio.

## ğŸ›  Tecnologias Utilizadas
- **Python**: Pandas, NumPy, Scikit-Learn, Plotly
- **SQL**
- **Jupyter Notebook**
- **API Flask/FastAPI** (para deploy)
- **Git & GitHub** (controle de versÃ£o e portfÃ³lio)

## ğŸƒâ€â™€ï¸â€â¡ï¸ Como Executar o Projeto
1. Clone o repositÃ³rio:
   ```bash
   git clone https://github.com/livgrigolon/7-days-of-data-science.git
   ```
2. Crie um ambiente virtual e instale as dependÃªncias:
   ```bash
   python -m venv venv
   source venv/bin/activate  # (ou venv\Scripts\activate no Windows)
   pip install -r requirements.txt
   ```
3. Execute os notebooks no Jupyter ou VS Code.

## ğŸŒŸ Autor
Desenvolvido por **LÃ­via Grigolon**, como parte do desafio [**7 Days of Code - CiÃªncia de Dados**](https://7daysofcode.io/matricula/data-science). Conecte-se comigo no [LinkedIn](https://www.linkedin.com/in/livgrigolon/).
